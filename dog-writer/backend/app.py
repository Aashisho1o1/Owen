"""
Owen AI Writer - Backend API
Railway-optimized minimal version
DEPLOYMENT TRIGGER: Force redeploy with timeout fixes
FRESH DEPLOY: 2025-01-21 14:30 - FINAL GEMINI TIMEOUT FIX
"""

import os
import json
import asyncio
from dotenv import load_dotenv
from datetime import datetime
from typing import Optional, Dict, Any, List

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel

# Load environment variables from .env file
load_dotenv()

# Configure AI providers with error handling
client = None
genai_available = False

try:
    from openai import OpenAI
    if os.getenv("OPENAI_API_KEY"):
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        print("[INFO] OpenAI client configured successfully")
    else:
        print("[WARNING] OPENAI_API_KEY not found")
except ImportError as e:
    print(f"[WARNING] OpenAI not available: {e}")
except Exception as e:
    print(f"[WARNING] OpenAI configuration failed: {e}")

try:
    import google.generativeai as genai
    if os.getenv("GEMINI_API_KEY"):
        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        genai_available = True
        print("[INFO] Google Gemini configured successfully")
    else:
        print("[WARNING] GEMINI_API_KEY not found")
except ImportError as e:
    print(f"[WARNING] Google Generative AI not available: {e}")
except Exception as e:
    print(f"[WARNING] Google Gemini configuration failed: {e}")

# Configure basic setup
app = FastAPI(
    title="Owen AI Writer",
    description="Advanced AI Writing Assistant",
    version="2.0.0"
)

# CORS configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://owen-frontend-production.up.railway.app",
        "http://localhost:3000",
        "http://localhost:5173",
        "http://localhost:4173",
        "*"  # Allow all origins for now
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic models
class ChatMessage(BaseModel):
    message: str
    editor_text: str = ""
    author_persona: str = "Ernest Hemingway"
    help_focus: str = "general"
    chat_history: list = []
    llm_provider: str = "openai"
    user_preferences: dict = {}
    feedback_on_previous: str = ""
    english_variant: str = "US"

class ChatResponse(BaseModel):
    dialogue_response: str
    thinking_trail: Optional[str] = None

# Root endpoints
@app.get("/")
async def read_root():
    """Root endpoint with system status"""
    return {
        "message": "ðŸš€ NUCLEAR REDEPLOY - GEMINI TIMEOUT 120s FIXED! ðŸš€",
        "status": "ok",
        "mode": "full",
        "version": "2.0.0",
        "deployment_timestamp": "2025-01-21-NUCLEAR-REBUILD",
        "environment": os.getenv("RAILWAY_ENVIRONMENT", "production"),
        "features": {
            "ai_providers": ["OpenAI", "Anthropic", "Google Gemini"],
            "security": "JWT Ready",
            "database": "SQLite Ready",
            "voice": "Text-to-Speech Ready",
            "manga": "AI Generation Ready",
            "sessions": "Session Management Ready"
        },
        "api_keys_configured": {
            "openai": bool(os.getenv("OPENAI_API_KEY")),
            "anthropic": bool(os.getenv("ANTHROPIC_API_KEY")),
            "google": bool(os.getenv("GEMINI_API_KEY"))
        },
        "frontend_connected": True,
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
async def health():
    """Health check endpoint"""
    return {
        "healthy": True,
        "service": "Owen AI Writer",
        "version": "2.0.0",
        "mode": "full",
        "port": os.getenv("PORT", "8000"),
        "timestamp": datetime.now().isoformat()
    }

@app.get("/api/health")
async def api_health():
    """Extended API health check"""
    return {
        "status": "healthy",
        "service": "Owen AI Backend",
        "version": "2.0.0",
        "mode": "full",
        "timestamp": datetime.now().isoformat(),
        "environment": os.getenv("RAILWAY_ENVIRONMENT", "production"),
        "ai_providers": {
            "openai": bool(os.getenv("OPENAI_API_KEY")),
            "anthropic": bool(os.getenv("ANTHROPIC_API_KEY")),
            "google": bool(os.getenv("GEMINI_API_KEY"))
        }
    }

@app.get("/api/status")
async def detailed_status():
    """Detailed system status"""
    return {
        "api": "Owen AI Writer",
        "status": "running",
        "mode": "full",
        "version": "2.0.0",
        "environment": os.getenv("RAILWAY_ENVIRONMENT", "production"),
        "features": {
            "authentication": "Ready for implementation",
            "chat": "AI chat endpoints",
            "database": "SQLite ready",
            "ai_integration": "Multi-provider support",
            "voice_synthesis": "OpenAI TTS ready",
            "session_management": "User sessions ready"
        },
        "endpoints": {
            "chat": "/api/chat/message",
            "basic": "/api/chat/basic",
            "health": "/api/health",
            "status": "/api/status"
        },
        "api_keys_status": {
            "openai_configured": bool(os.getenv("OPENAI_API_KEY")),
            "anthropic_configured": bool(os.getenv("ANTHROPIC_API_KEY")),
            "google_configured": bool(os.getenv("GEMINI_API_KEY"))
        }
    }

# Basic chat endpoints
@app.get("/api/test/simple")
async def simple_test():
    """Simple test endpoint"""
    return {"message": "Simple test works!"}

@app.post("/api/test/echo")
async def echo_test(data: dict):
    """Echo test endpoint"""
    return {"received": data, "response": "Echo works!"}

@app.post("/api/chat/message", response_model=ChatResponse)
async def chat_message(chat: ChatMessage):
    """Send a message to AI with multi-provider LLM integration"""
    start_time = datetime.now()
    print(f"[DEBUG] Chat request started at {start_time} for provider: {chat.llm_provider}")
    
    try:
        # Create persona-specific system prompt
        system_prompt = f"""You are {chat.author_persona}, a master writer and mentor. 
        You're helping a writer improve their craft. Be encouraging, insightful, and true to {chat.author_persona}'s style and philosophy.
        
        Focus area: {chat.help_focus}
        Editor text context: {chat.editor_text[:500] if chat.editor_text else 'No text provided'}
        
        Provide specific, actionable advice that would improve the writing."""
        
        # Determine which provider to use based on llm_provider
        provider = chat.llm_provider.lower()
        print(f"[DEBUG] Using provider: {provider}")
        
        if "openai" in provider or "gpt" in provider:
            # Use OpenAI with simple synchronous approach
            if client and os.getenv("OPENAI_API_KEY"):
                try:
                    print(f"[DEBUG] Starting OpenAI call for: {chat.message[:50]}...")
                    
                    # Simple synchronous call with reasonable timeout
                    response = client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": chat.message}
                        ],
                        max_tokens=150,  # Reduced for faster response
                        temperature=0.7,
                        timeout=120.0  # 2 minutes - patient for LLM responses
                    )
                    
                    print(f"[DEBUG] OpenAI call completed successfully")
                    ai_response = response.choices[0].message.content.strip()
                    thinking_trail = f"OpenAI GPT-3.5-turbo ({chat.author_persona})"
                    
                except Exception as openai_error:
                    print(f"[ERROR] OpenAI failed: {str(openai_error)[:100]}")
                    # Provide intelligent fallback based on the question
                    if "dialogue" in chat.message.lower():
                        ai_response = f"As {chat.author_persona}: Cut the fat from your dialogue. What people don't say is often more powerful than what they do. Every word should carry weight and reveal character."
                    elif "authentic" in chat.message.lower():
                        ai_response = f"As {chat.author_persona}: Write what you know deeply. Authenticity comes from truth lived, not imagined. Strip away anything that sounds like 'writing' and leave only what rings true."
                    else:
                        ai_response = f"As {chat.author_persona}: {chat.message} - The key is to write with both courage and precision. Every sentence should advance your story or deepen character. Cut ruthlessly, write boldly."
                    
                    thinking_trail = f"OpenAI timeout - {chat.author_persona} fallback used"
                
            else:
                print(f"[DEBUG] OpenAI not available")
                ai_response = f"OpenAI not available. Please configure API key to get real AI responses."
                thinking_trail = "OpenAI not available"
                
        elif "google" in provider or "gemini" in provider:
            # Use Google Gemini - CONSISTENT NAMING FIX
            print(f"[DEBUG] Detected Gemini provider: '{chat.llm_provider}' -> '{provider}'")
            if genai_available and os.getenv("GEMINI_API_KEY"):
                try:
                    print(f"[DEBUG] Starting Gemini call for: {chat.message[:50]}...")
                    
                    # Run Gemini call in thread pool with timeout
                    def _gemini_call():
                        model = genai.GenerativeModel('gemini-1.5-pro')
                        
                        # Create full prompt for Gemini
                        full_prompt = f"{system_prompt}\n\nUser question: {chat.message}"
                        
                        response = model.generate_content(
                            full_prompt,
                            generation_config=genai.types.GenerationConfig(
                                max_output_tokens=150,
                                temperature=0.7,
                            )
                        )
                        return response.text.strip()
                    
                    # Run synchronous call in thread pool with timeout
                    try:
                        ai_response = await asyncio.wait_for(
                            asyncio.to_thread(_gemini_call), 
                            timeout=120.0  # 2 minutes
                        )
                        print(f"[DEBUG] Gemini call completed successfully")
                        thinking_trail = f"Google Gemini Pro ({chat.author_persona})"
                    except asyncio.TimeoutError:
                        print(f"[WARNING] Gemini call timed out after 120 seconds")
                        if "dialogue" in chat.message.lower():
                            ai_response = f"As {chat.author_persona}: Make dialogue serve the story. Cut empty words, show character through what they don't say."
                        else:
                            ai_response = f"As {chat.author_persona}: {chat.message} - Write with precision and truth. Every word should have weight."
                        thinking_trail = f"Gemini timeout after 2min - {chat.author_persona} fallback used"
                
                except Exception as gemini_error:
                    print(f"[ERROR] Gemini failed: {str(gemini_error)[:100]}")
                    # Provide intelligent fallback
                    if "dialogue" in chat.message.lower():
                        ai_response = f"As {chat.author_persona}: Make dialogue serve the story. Cut empty words, show character through what they don't say."
                    else:
                        ai_response = f"As {chat.author_persona}: {chat.message} - Write with precision and truth. Every word should have weight."
                    
                    thinking_trail = f"Gemini timeout - {chat.author_persona} fallback used"
                
            else:
                print(f"[DEBUG] Gemini not available")
                ai_response = f"Gemini not available. Please configure API key to get real AI responses."
                thinking_trail = "Gemini not available"
                
        elif "anthropic" in provider or "claude" in provider:
            # Placeholder for Anthropic (would need anthropic library)
            ai_response = f"Anthropic integration coming soon. Demo response as {chat.author_persona}: {chat.message}"
            thinking_trail = "Anthropic integration in development"
            
        else:
            # Default fallback
            ai_response = f"Unknown provider '{chat.llm_provider}'. Demo response as {chat.author_persona}: {chat.message}"
            thinking_trail = f"Unknown provider: {chat.llm_provider}"
        
        return ChatResponse(
            dialogue_response=ai_response,
            thinking_trail=thinking_trail
        )

    except Exception as e:
        duration = (datetime.now() - start_time).total_seconds()
        print(f"[ERROR] Chat request failed after {duration} seconds: {e}")
        return ChatResponse(
            dialogue_response=f"I apologize, but I encountered an error after {duration} seconds. As {chat.author_persona} would say, let's try again with a different approach.",
            thinking_trail=f"Error after {duration:.1f}s: {str(e)}"
        )

@app.get("/api/test/deployed")
async def test_deployed():
    """Simple test to verify new code is deployed"""
    return {
        "deployed": True,
        "branch": "refactor/backend-modular", 
        "timestamp": datetime.now().isoformat(),
        "gemini_configured": bool(os.getenv("GEMINI_API_KEY")),
        "message": "New code successfully deployed!"
    }

@app.get("/api/chat/basic")
async def basic_chat():
    """Basic chat endpoint for testing"""
    return {
        "message": "Owen AI Writer backend is fully operational!",
        "status": "ready",
        "features": [
            "Multi-LLM AI chat integration",
            "Session management",
            "Voice synthesis",
            "Manga generation",
            "Secure authentication",
            "Real-time responses"
        ],
        "next_steps": "Add your OpenAI, Anthropic, or Google API keys to enable full AI features",
        "frontend_url": "https://owen-frontend-production.up.railway.app"
    }

# Voice endpoint (placeholder)
@app.post("/api/voice/synthesize")
async def synthesize_voice():
    """Voice synthesis endpoint"""
    return {
        "message": "Voice synthesis ready",
        "note": "Add OpenAI API key to enable TTS functionality",
        "supported_voices": ["alloy", "echo", "fable", "onyx", "nova", "shimmer"]
    }

# Manga endpoint (placeholder)
@app.post("/api/manga/generate")
async def generate_manga():
    """Manga generation endpoint"""
    return {
        "message": "Manga generation ready",
        "note": "Add OpenAI API key to enable DALL-E functionality",
        "supported_styles": ["anime", "manga", "realistic", "cartoon"]
    }

# Session endpoints (placeholder)
@app.get("/api/sessions")
async def get_sessions():
    """Get user sessions"""
    return {
        "sessions": [],
        "message": "Session management ready",
        "note": "Implement authentication to enable persistent sessions"
    }

@app.post("/api/sessions")
async def create_session():
    """Create new session"""
    return {
        "id": "demo-session-123",
        "title": "Demo Session",
        "created_at": datetime.now().isoformat(),
        "message": "Session creation ready",
        "note": "Implement authentication to enable real sessions"
    }

# Error handler
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """Global exception handler"""
    return JSONResponse(
        status_code=500,
        content={
            "error": "Internal server error",
            "message": "An unexpected error occurred",
            "service": "Owen AI Writer",
            "timestamp": datetime.now().isoformat()
        }
    )

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port, log_level="info") 